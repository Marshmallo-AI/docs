1:"$Sreact.fragment"
3:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"default"]
4:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"default"]
5:I[55169,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"TOCProvider"]
6:I[769,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"Sidebar"]
7:I[47486,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ClientWrapper"]
d:I[68027,[],"default"]
:HL["/_next/static/chunks/221ce6682042a4f8.css","style"]
8:Tbe2,---
sidebar_position: 2
---

# Rewards

Rewards evaluate how well your agent completed a task. They turn a raw trace into a score and explanation, telling you what worked, what failed, and why.

## Why Rewards Matter

Agents can fail in subtle ways. A response might look correct but miss key details. A tool might return data that the agent misinterprets. Without structured evaluation, these failures go unnoticed.

Rewards solve this by grading every task against consistent criteria. You get a numeric score that tracks quality over time, plus a written rationale that explains the reasoning. This makes failures visible and actionable.

Rewards also feed into Marlo's learning system. When a task receives a low score, Marlo uses the rationale to generate guidance that prevents similar failures in the future.

## How Rewards Work

Rewards are computed automatically after a task ends:

1. **Trace Collection**: Marlo gathers the complete trace for the task, including the agent definition, user input, LLM calls, tool calls, and final output.

2. **Evaluation**: A judge model reviews the trace and evaluates the outcome based on task completion, response quality, and tool usage.

3. **Scoring**: The judge assigns a score between 0.0 and 1.0, where 0.0 means complete failure and 1.0 means perfect execution.

4. **Rationale**: The judge writes a short explanation describing what the agent did well and what it did poorly, with references to specific steps in the trace.

## What Rewards Evaluate

The judge considers several factors when scoring a task:

- **Task Completion**: Did the agent accomplish what the user asked for?
- **Response Quality**: Is the final output accurate, relevant, and well-formed?
- **Tool Usage**: Did the agent use tools correctly and interpret their outputs properly?
- **Efficiency**: Did the agent complete the task without unnecessary steps or repeated calls?
- **Error Handling**: If something went wrong, did the agent recover gracefully or fail silently?

## What You Get

Each reward includes:

- **Score**: A number between 0.0 and 1.0 representing overall task quality.
- **Rationale**: A written explanation of the score, highlighting strengths and weaknesses.
- **Evidence**: Links to specific events in the trace that support the evaluation.

## Viewing Rewards in the Dashboard

You can view rewards for any task in the Marlo dashboard:

1. Navigate to your project and select a thread.
2. Click on a task to open the trace view.
3. The reward score and rationale appear alongside the trace events.
4. Click on evidence links to jump directly to the relevant steps.

Over time, you can track reward trends to see whether your agent is improving or degrading.

## Connection to Learnings

Rewards are the input to Marlo's learning system. When a task receives a reward, Marlo analyzes the rationale and generates learning objects that capture what the agent should do differently. These learnings are then surfaced in future tasks, allowing the agent to avoid repeating the same mistakes.0:{"P":null,"b":"MfaPKFdOo7L2xMG-Jwysa","c":["","features","rewards"],"q":"","i":false,"f":[[["",{"children":["features",{"children":["rewards",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/221ce6682042a4f8.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/ff1a16fafef87110.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/014d3a509c2c44a0.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/_next/static/chunks/3a4fb10029fc91d1.js","async":true,"nonce":"$undefined"}],["$","script","script-3",{"src":"/_next/static/chunks/e0afa020722afb62.js","async":true,"nonce":"$undefined"}]],"$L2"]}],{"children":[["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"className":"x:mx-auto x:flex x:max-w-(--nextra-content-width)","children":["$","$L5",null,{"value":[{"value":"Why Rewards Matter","id":"why-rewards-matter","depth":2},{"value":"How Rewards Work","id":"how-rewards-work","depth":2},{"value":"What Rewards Evaluate","id":"what-rewards-evaluate","depth":2},{"value":"What You Get","id":"what-you-get","depth":2},{"value":"Viewing Rewards in the Dashboard","id":"viewing-rewards-in-the-dashboard","depth":2},{"value":"Connection to Learnings","id":"connection-to-learnings","depth":2}],"children":[["$","$L6",null,{}],["$","$L7",null,{"metadata":{"sidebar_position":2,"title":"Rewards","filePath":"src/app/features/rewards/page.mdx","timestamp":1769464301000},"bottomContent":"$undefined","sourceCode":"$8","children":["$L9","$La"]}]]}]}],null,"$Lb"]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$Lc",false]],"m":"$undefined","G":["$d",[]],"S":true}
e:I[63894,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"HeadingAnchor"]
19:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"OutletBoundary"]
1a:"$Sreact.suspense"
1c:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"ViewportBoundary"]
1e:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"MetadataBoundary"]
9:["$","div",null,{"id":"nextra-skip-nav"}]
a:["$","main",null,{"data-pagefind-body":true,"children":[["$","h1",null,{"id":"$undefined","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-bold x:mt-2 x:text-4xl","children":["Rewards","$undefined"]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Rewards evaluate how well your agent completed a task. They turn a raw trace into a score and explanation, telling you what worked, what failed, and why."}],"\n",["$","h2",null,{"id":"why-rewards-matter","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Why Rewards Matter",["$","$Le",null,{"id":"why-rewards-matter"}]]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Agents can fail in subtle ways. A response might look correct but miss key details. A tool might return data that the agent misinterprets. Without structured evaluation, these failures go unnoticed."}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Rewards solve this by grading every task against consistent criteria. You get a numeric score that tracks quality over time, plus a written rationale that explains the reasoning. This makes failures visible and actionable."}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Rewards also feed into Marlo’s learning system. When a task receives a low score, Marlo uses the rationale to generate guidance that prevents similar failures in the future."}],"\n",["$","h2",null,{"id":"how-rewards-work","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["How Rewards Work",["$","$Le",null,{"id":"how-rewards-work"}]]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Rewards are computed automatically after a task ends:"}],"\n",["$","ol",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-decimal x:ms-6","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":["\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":[["$","strong",null,{"children":"Trace Collection"}],": Marlo gathers the complete trace for the task, including the agent definition, user input, LLM calls, tool calls, and final output."]}],"\n"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":["\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":[["$","strong",null,{"children":"Evaluation"}],": A judge model reviews the trace and evaluates the outcome based on task completion, response quality, and tool usage."]}],"\n"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":["\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":[["$","strong",null,{"children":"Scoring"}],": The judge assigns a score between 0.0 and 1.0, where 0.0 means complete failure and 1.0 means perfect execution."]}],"\n"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":["\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":[["$","strong",null,{"children":"Rationale"}],": The judge writes a short explanation describing what the agent did well and what it did poorly, with references to specific steps in the trace."]}],"\n"]}],"\n"]}],"\n",["$","h2",null,{"id":"what-rewards-evaluate","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["What Rewards Evaluate",["$","$Le",null,{"id":"what-rewards-evaluate"}]]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"The judge considers several factors when scoring a task:"}],"\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18"]}]
b:["$","$L19",null,{"children":["$","$1a",null,{"name":"Next.MetadataOutlet","children":"$@1b"}]}]
c:["$","$1","h",{"children":[null,["$","$L1c",null,{"children":"$L1d"}],["$","div",null,{"hidden":true,"children":["$","$L1e",null,{"children":["$","$1a",null,{"name":"Next.Metadata","children":"$L1f"}]}]}],null]}]
f:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Task Completion"}],": Did the agent accomplish what the user asked for?"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Response Quality"}],": Is the final output accurate, relevant, and well-formed?"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Tool Usage"}],": Did the agent use tools correctly and interpret their outputs properly?"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Efficiency"}],": Did the agent complete the task without unnecessary steps or repeated calls?"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Error Handling"}],": If something went wrong, did the agent recover gracefully or fail silently?"]}],"\n"]}]
10:["$","h2",null,{"id":"what-you-get","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["What You Get",["$","$Le",null,{"id":"what-you-get"}]]}]
11:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Each reward includes:"}]
12:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Score"}],": A number between 0.0 and 1.0 representing overall task quality."]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Rationale"}],": A written explanation of the score, highlighting strengths and weaknesses."]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"Evidence"}],": Links to specific events in the trace that support the evaluation."]}],"\n"]}]
13:["$","h2",null,{"id":"viewing-rewards-in-the-dashboard","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Viewing Rewards in the Dashboard",["$","$Le",null,{"id":"viewing-rewards-in-the-dashboard"}]]}]
14:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"You can view rewards for any task in the Marlo dashboard:"}]
15:["$","ol",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-decimal x:ms-6","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":"Navigate to your project and select a thread."}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"Click on a task to open the trace view."}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"The reward score and rationale appear alongside the trace events."}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"Click on evidence links to jump directly to the relevant steps."}],"\n"]}]
16:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Over time, you can track reward trends to see whether your agent is improving or degrading."}]
17:["$","h2",null,{"id":"connection-to-learnings","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Connection to Learnings",["$","$Le",null,{"id":"connection-to-learnings"}]]}]
18:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Rewards are the input to Marlo’s learning system. When a task receives a reward, Marlo analyzes the rationale and generates learning objects that capture what the agent should do differently. These learnings are then surfaced in future tasks, allowing the agent to avoid repeating the same mistakes."}]
20:I[68386,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ThemeConfigProvider"]
21:I[6344,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"LastUpdated"]
22:I[98091,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"Search"]
23:I[63178,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ThemeProvider"]
24:I[91675,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"SkipNavLink"]
25:I[37985,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ConfigProvider"]
2:["$","html",null,{"lang":"en","dir":"ltr","suppressHydrationWarning":true,"children":[["$","head",null,{"children":["$undefined",["$","style",null,{"children":":root {\n  --nextra-primary-hue: 212deg;\n  --nextra-primary-saturation: 100%;\n  --nextra-primary-lightness: 45%;\n  --nextra-bg: 250,250,250;\n  --nextra-content-width: 90rem;\n}\n.dark {\n  --nextra-primary-hue: 204deg;\n  --nextra-primary-saturation: 100%;\n  --nextra-primary-lightness: 55%;\n  --nextra-bg: 17,17,17;\n}\n::selection {\n  background: hsla(var(--nextra-primary-hue),var(--nextra-primary-saturation),var(--nextra-primary-lightness),.3);\n}\nhtml {\n  background: rgb(var(--nextra-bg));\n}"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"rgb(250,250,250)"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"rgb(17,17,17)"}],"$undefined"]}],["$","body",null,{"children":["$","$L20",null,{"value":{"copyPageButton":true,"darkMode":true,"docsRepositoryBase":"https://github.com/shuding/nextra/tree/main/docs","editLink":"Edit this page","feedback":{"content":"Question? Give us feedback","labels":"feedback"},"i18n":[],"lastUpdated":["$","$L21",null,{}],"navigation":{"next":true,"prev":true},"search":["$","$L22",null,{}],"sidebar":{"defaultMenuCollapseLevel":2,"defaultOpen":true,"toggleButton":true},"themeSwitch":{"dark":"Dark","light":"Light","system":"System"},"toc":{"backToTop":"Scroll to top","float":true,"title":"On This Page"}},"children":["$","$L23",null,{"attribute":"class","defaultTheme":"system","disableTransitionOnChange":true,"storageKey":"theme","children":[["$","$L24",null,{}],"$undefined",["$","$L25",null,{"pageMap":[{"name":"index","route":"/","frontMatter":{"sidebar_position":1,"slug":"/","title":"Introduction to Marlo","filePath":"src/app/page.mdx","timestamp":1769464301000},"title":"Introduction to Marlo"},{"name":"getting-started","route":"/getting-started","frontMatter":{"sidebar_position":2,"title":"Getting Started","filePath":"src/app/getting-started/page.mdx","timestamp":1769464301000},"title":"Getting Started"},{"name":"sdk","route":"/sdk","children":[{"name":"python","route":"/sdk/python","frontMatter":{"sidebar_position":1,"title":"Python","filePath":"src/app/sdk/python/page.mdx","timestamp":1769464301000},"title":"Python"},{"name":"typescript","route":"/sdk/typescript","frontMatter":{"sidebar_position":2,"title":"TypeScript","filePath":"src/app/sdk/typescript/page.mdx","timestamp":1769464301000},"title":"TypeScript"},{"name":"http","route":"/sdk/http","frontMatter":{"sidebar_position":3,"title":"HTTP API","filePath":"src/app/sdk/http/page.mdx","timestamp":1769464301000},"title":"HTTP API"}],"title":"Sdk"},{"name":"features","route":"/features","children":[{"name":"tracing","route":"/features/tracing","frontMatter":{"sidebar_position":1,"title":"Tracing","filePath":"src/app/features/tracing/page.mdx","timestamp":1769464301000},"title":"Tracing"},{"name":"rewards","route":"/features/rewards","frontMatter":"$0:f:0:1:1:children:1:children:1:children:0:props:children:0:props:children:props:children:1:props:metadata","title":"Rewards"},{"name":"learning","route":"/features/learning","frontMatter":{"sidebar_position":3,"title":"Learnings","filePath":"src/app/features/learning/page.mdx","timestamp":1769464301000},"title":"Learnings"},{"name":"copilot","route":"/features/copilot","frontMatter":{"sidebar_position":4,"title":"Co-pilot","filePath":"src/app/features/copilot/page.mdx","timestamp":1769464301000},"title":"Co-pilot"},{"name":"simulation","route":"/features/simulation","frontMatter":{"sidebar_position":5,"title":"Simulation","filePath":"src/app/features/simulation/page.mdx","timestamp":1769464301000},"title":"Simulation"},{"name":"reports","route":"/features/reports","frontMatter":{"sidebar_position":6,"title":"Reports","filePath":"src/app/features/reports/page.mdx","timestamp":1769464301000},"title":"Reports"}],"title":"Features"},{"name":"agent-systems","route":"/agent-systems","children":[{"name":"single-agent","route":"/agent-systems/single-agent","frontMatter":{"sidebar_position":1,"title":"Single Agent","filePath":"src/app/agent-systems/single-agent/page.mdx","timestamp":1769464301000},"title":"Single Agent"},{"name":"orchestrator","route":"/agent-systems/orchestrator","frontMatter":{"sidebar_position":2,"title":"Orchestrator","filePath":"src/app/agent-systems/orchestrator/page.mdx","timestamp":1769464301000},"title":"Orchestrator"},{"name":"chain","route":"/agent-systems/chain","frontMatter":{"sidebar_position":3,"title":"Chain","filePath":"src/app/agent-systems/chain/page.mdx","timestamp":1769464301000},"title":"Chain"},{"name":"graph","route":"/agent-systems/graph","frontMatter":{"sidebar_position":4,"title":"Graph","filePath":"src/app/agent-systems/graph/page.mdx","timestamp":1769464301000},"title":"Graph"}],"title":"Agent Systems"}],"navbar":"$L26","footer":"$L27","children":["$L28","$L29"]}]]}]}]}]]}]
2a:I[22016,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],""]
2b:I[43634,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ClientNavbar"]
2c:I[31277,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"Switchers"]
2d:I[34924,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"LocaleSwitch"]
2e:I[75684,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ThemeSwitch"]
2f:I[769,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"MobileNav"]
26:["$","header",null,{"className":"nextra-navbar x:sticky x:top-0 x:z-30 x:w-full x:bg-transparent x:print:hidden x:max-md:[.nextra-banner:not([class$=hidden])~&]:top-(--nextra-banner-height)","children":[["$","div",null,{"className":"nextra-navbar-blur x:absolute x:-z-1 x:size-full nextra-border x:border-b x:backdrop-blur-md x:bg-nextra-bg/70"}],["$","nav",null,{"style":{"height":"var(--nextra-navbar-height)"},"className":"x:mx-auto x:flex x:max-w-(--nextra-content-width) x:items-center x:gap-4 x:pl-[max(env(safe-area-inset-left),1.5rem)] x:pr-[max(env(safe-area-inset-right),1.5rem)] x:justify-end","children":[["$","$L2a",null,{"href":"/","className":"x:flex x:items-center x:me-auto x:transition-opacity x:focus-visible:nextra-focus x:hover:opacity-75","aria-label":"Home page","children":["$","b",null,{"children":"Marshmallo"}]}],["$","$L2b",null,{"className":"","children":["$undefined","$undefined","$undefined"]}]]}]]}]
27:["$","div",null,{"className":"x:bg-gray-100 x:pb-[env(safe-area-inset-bottom)] x:dark:bg-neutral-900 x:print:bg-transparent","children":[["$","$L2c",null,{"children":["$","div",null,{"className":"x:mx-auto x:flex x:max-w-(--nextra-content-width) x:gap-2 x:py-2 x:px-4","children":[["$","$L2d",null,{}],["$","$L2e",null,{}]]}]}],["$","hr",null,{"className":"nextra-border"}],["$","footer",null,{"className":"x:mx-auto x:flex x:max-w-(--nextra-content-width) x:justify-center x:py-12 x:text-gray-600 x:dark:text-gray-400 x:md:justify-start x:pl-[max(env(safe-area-inset-left),1.5rem)] x:pr-[max(env(safe-area-inset-right),1.5rem)]","children":["MIT ",2026," © Marshmallo."]}]]}]
28:["$","$L2f",null,{}]
29:["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]
1d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
30:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"IconMark"]
1b:null
1f:[["$","title","0",{"children":"Rewards"}],["$","link","1",{"rel":"icon","href":"/favicon.ico?favicon.0b3bf435.ico","sizes":"256x256","type":"image/x-icon"}],["$","$L30","2",{}]]
