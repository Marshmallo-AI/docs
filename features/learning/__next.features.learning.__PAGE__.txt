1:"$Sreact.fragment"
2:I[55169,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"TOCProvider"]
3:I[769,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"Sidebar"]
4:I[47486,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ClientWrapper"]
6:I[63894,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"HeadingAnchor"]
22:I[83987,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/014d3a509c2c44a0.js","/_next/static/chunks/3a4fb10029fc91d1.js","/_next/static/chunks/e0afa020722afb62.js"],"ToggleWordWrapButton"]
23:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/7340adf74ff47ec0.js"],"OutletBoundary"]
24:"$Sreact.suspense"
5:T720,---
sidebar_position: 3
---

# Learning

Learning turns reward rationales into stable, reusable guidance for the agent.
The goal is simple: **reduce failures and keep decisions consistent**.

## Inputs To Learning
Learning runs after rewards. It uses:
- task reward output (score + rationale + principles)
- task trajectory context (same task the judge saw)
- agent definition (system prompt and tools)

Learning never runs on guesses. It only uses what was actually captured.

## What A Learning Is
Each learning is stored as a small, structured rule:

- **cue**: what to detect in the task or context
- **action**: what the agent should do
- **expected effect**: why this helps

This format makes learnings easy to apply and to measure.

## Shadow vs Active
Learnings start in **shadow**:
- they are tracked
- they are not injected into the prompt

When a learning proves useful over time, it moves to **active**:
- it is injected into the prompt
- its adoption and outcomes are tracked

If it harms performance, it can be demoted back to shadow.

## How We Know A Learning Works
Marlo tracks adoption and outcomes:
- did the agent follow the learning?
- did reward scores improve?
- did failures drop?

If adoption is low or outcomes degrade, the learning is removed or demoted.

## Where Learnings Live
Learnings are stored per agent (using `learning_key`, usually the `agent_id`).
That means each agent has its own learning set.

In multi‑agent systems, each agent has separate learnings.

## How Learnings Are Used
The SDK returns learnings as text.
You inject them into your system prompt.

Example:
```python
learnings_text = marlo.get_learnings_text()

SYSTEM_PROMPT = f"""
You are a support agent.

Learnings:
{learnings_text}
"""
```

If you do not inject learnings, they exist but they do not affect the agent.0:{"buildId":"wrVKmalm7spyZDZ7lPwrU","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"x:mx-auto x:flex x:max-w-(--nextra-content-width)","children":["$","$L2",null,{"value":[{"value":"Inputs To Learning","id":"inputs-to-learning","depth":2},{"value":"What A Learning Is","id":"what-a-learning-is","depth":2},{"value":"Shadow vs Active","id":"shadow-vs-active","depth":2},{"value":"How We Know A Learning Works","id":"how-we-know-a-learning-works","depth":2},{"value":"Where Learnings Live","id":"where-learnings-live","depth":2},{"value":"How Learnings Are Used","id":"how-learnings-are-used","depth":2}],"children":[["$","$L3",null,{}],["$","$L4",null,{"metadata":{"sidebar_position":3,"title":"Learning","filePath":"src/app/features/learning/page.mdx","timestamp":1767344856000},"sourceCode":"$5","children":[["$","div",null,{"id":"nextra-skip-nav"}],["$","main",null,{"data-pagefind-body":true,"children":[["$","h1",null,{"className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-bold x:mt-2 x:text-4xl","children":["Learning","$undefined"]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":["Learning turns reward rationales into stable, reusable guidance for the agent.\nThe goal is simple: ",["$","strong",null,{"children":"reduce failures and keep decisions consistent"}],"."]}],"\n",["$","h2",null,{"id":"inputs-to-learning","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Inputs To Learning",["$","$L6",null,{"id":"inputs-to-learning"}]]}],"\n",["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Learning runs after rewards. It uses:"}],"\n",["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n","$L7","\n","$L8","\n","$L9","\n"]}],"\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20"]}]]}]]}]}],null,"$L21"]}],"loading":null,"isPartial":false}
7:["$","li",null,{"className":"x:my-[.5em]","children":"task reward output (score + rationale + principles)"}]
8:["$","li",null,{"className":"x:my-[.5em]","children":"task trajectory context (same task the judge saw)"}]
9:["$","li",null,{"className":"x:my-[.5em]","children":"agent definition (system prompt and tools)"}]
a:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Learning never runs on guesses. It only uses what was actually captured."}]
b:["$","h2",null,{"id":"what-a-learning-is","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["What A Learning Is",["$","$L6",null,{"id":"what-a-learning-is"}]]}]
c:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Each learning is stored as a small, structured rule:"}]
d:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"cue"}],": what to detect in the task or context"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"action"}],": what the agent should do"]}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":[["$","strong",null,{"children":"expected effect"}],": why this helps"]}],"\n"]}]
e:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"This format makes learnings easy to apply and to measure."}]
f:["$","h2",null,{"id":"shadow-vs-active","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Shadow vs Active",["$","$L6",null,{"id":"shadow-vs-active"}]]}]
10:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":["Learnings start in ",["$","strong",null,{"children":"shadow"}],":"]}]
11:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":"they are tracked"}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"they are not injected into the prompt"}],"\n"]}]
12:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":["When a learning proves useful over time, it moves to ",["$","strong",null,{"children":"active"}],":"]}]
13:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":"it is injected into the prompt"}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"its adoption and outcomes are tracked"}],"\n"]}]
14:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"If it harms performance, it can be demoted back to shadow."}]
15:["$","h2",null,{"id":"how-we-know-a-learning-works","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["How We Know A Learning Works",["$","$L6",null,{"id":"how-we-know-a-learning-works"}]]}]
16:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Marlo tracks adoption and outcomes:"}]
17:["$","ul",null,{"className":"x:[:is(ol,ul)_&]:my-[.75em] x:not-first:mt-[1.25em] x:list-disc x:ms-[1.5em]","children":["\n",["$","li",null,{"className":"x:my-[.5em]","children":"did the agent follow the learning?"}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"did reward scores improve?"}],"\n",["$","li",null,{"className":"x:my-[.5em]","children":"did failures drop?"}],"\n"]}]
18:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"If adoption is low or outcomes degrade, the learning is removed or demoted."}]
19:["$","h2",null,{"id":"where-learnings-live","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["Where Learnings Live",["$","$L6",null,{"id":"where-learnings-live"}]]}]
1a:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":["Learnings are stored per agent (using ",["$","code",null,{"className":"nextra-code","dir":"ltr","children":"learning_key"}],", usually the ",["$","code",null,{"className":"nextra-code","dir":"ltr","children":"agent_id"}],").\nThat means each agent has its own learning set."]}]
1b:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"In multi‑agent systems, each agent has separate learnings."}]
1c:["$","h2",null,{"id":"how-learnings-are-used","className":"x:tracking-tight x:text-slate-900 x:dark:text-slate-100 x:font-semibold x:target:animate-[fade-in_1.5s] x:mt-10 x:border-b x:pb-1 x:text-3xl nextra-border","children":["How Learnings Are Used",["$","$L6",null,{"id":"how-learnings-are-used"}]]}]
1d:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"The SDK returns learnings as text.\nYou inject them into your system prompt."}]
1e:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"Example:"}]
1f:["$","div",null,{"data-pagefind-ignore":"all","className":"nextra-code x:relative x:not-first:mt-[1.25em]","children":["$undefined",["$","pre",null,{"className":"x:group x:focus-visible:nextra-focus x:overflow-x-auto x:subpixel-antialiased x:text-[.9em] x:bg-white x:dark:bg-black x:py-4 x:ring-1 x:ring-inset x:ring-gray-300 x:dark:ring-neutral-700 x:contrast-more:ring-gray-900 x:contrast-more:dark:ring-gray-50 x:contrast-more:contrast-150 x:rounded-md not-prose","tabIndex":"0","children":[["$","div",null,{"className":"x:group-hover:opacity-100 x:group-focus:opacity-100 x:opacity-0 x:transition x:focus-within:opacity-100 x:flex x:gap-1 x:absolute x:right-4 x:top-2","children":[["$","$L22",null,{"children":["$","svg",null,{"viewBox":"0 0 24 24","fill":"currentColor","height":"1em","children":["$","path",null,{"d":"M4 19h6v-2H4v2zM20 5H4v2h16V5zm-3 6H4v2h13.25c1.1 0 2 .9 2 2s-.9 2-2 2H15v-2l-3 3l3 3v-2h2c2.21 0 4-1.79 4-4s-1.79-4-4-4z"}]}]}],false]}],["$","code",null,{"className":"nextra-code","dir":"ltr","children":[["$","span",null,{"children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"learnings_text "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" marlo.get_learnings_text()"}]]}],"\n",["$","span",null,{"children":" "}],"\n",["$","span",null,{"children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"SYSTEM_PROMPT"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" ="}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" f"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"\"\""}]]}],"\n",["$","span",null,{"children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"You are a support agent."}]}],"\n",["$","span",null,{"children":" "}],"\n",["$","span",null,{"children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"Learnings:"}]}],"\n",["$","span",null,{"children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"{"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"learnings_text"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"}"}]]}],"\n",["$","span",null,{"children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"\"\""}]}]]}]]}]]}]
20:["$","p",null,{"className":"x:not-first:mt-[1.25em] x:leading-7","children":"If you do not inject learnings, they exist but they do not affect the agent."}]
21:["$","$L23",null,{"children":["$","$24",null,{"name":"Next.MetadataOutlet","children":"$@25"}]}]
25:null
