"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[176],{2354(e,s,r){r.r(s),r.d(s,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>n,toc:()=>o});const n=JSON.parse('{"id":"features/rewards","title":"Rewards","description":"Rewards are task\u2011level evaluations of what happened in a trace.","source":"@site/docs/features/rewards.md","sourceDirName":"features","slug":"/features/rewards","permalink":"/features/rewards","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/features/rewards.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Tracing","permalink":"/features/tracing"},"next":{"title":"Learning","permalink":"/features/learning"}}');var a=r(4848),d=r(8453);const t={sidebar_position:2},i="Rewards",l={},o=[{value:"What The Judge Sees",id:"what-the-judge-sees",level:2},{value:"What The Judge Returns",id:"what-the-judge-returns",level:2},{value:"Task\u2011Level Only",id:"tasklevel-only",level:2},{value:"Long Tasks And Memory",id:"long-tasks-and-memory",level:2},{value:"Reasoning Usage",id:"reasoning-usage",level:2},{value:"When Rewards Run",id:"when-rewards-run",level:2}];function c(e){const s={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,d.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(s.header,{children:(0,a.jsx)(s.h1,{id:"rewards",children:"Rewards"})}),"\n",(0,a.jsxs)(s.p,{children:["Rewards are task\u2011level evaluations of what happened in a trace.\nThey answer one question: ",(0,a.jsx)(s.strong,{children:"did this task succeed, and why?"})]}),"\n",(0,a.jsx)(s.h2,{id:"what-the-judge-sees",children:"What The Judge Sees"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsx)(s.p,{children:"For each task, the judge reads:"}),(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"the task text"}),"\n",(0,a.jsx)(s.li,{children:"all task events (LLM calls, tool calls, logs)"}),"\n",(0,a.jsx)(s.li,{children:"the final answer"}),"\n",(0,a.jsx)(s.li,{children:"the agent definition (system prompt, tools, MCP, model config)"}),"\n",(0,a.jsx)(s.li,{children:"the agent tree (root + sub\u2011agents)"}),"\n"]}),(0,a.jsx)(s.p,{children:"If any of these are missing, the reward is less reliable."})]}),"\n",(0,a.jsx)(s.h2,{id:"what-the-judge-returns",children:"What The Judge Returns"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsx)(s.p,{children:"Rewards include:"}),(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"a score (numeric)"}),"\n",(0,a.jsx)(s.li,{children:"a rationale (why it scored that way)"}),"\n",(0,a.jsx)(s.li,{children:"principles (what was good or bad)"}),"\n",(0,a.jsx)(s.li,{children:"uncertainty (how confident the judge is)"}),"\n"]}),(0,a.jsx)(s.p,{children:"This output is stored with the task and used by learning."})]}),"\n",(0,a.jsx)(s.h2,{id:"tasklevel-only",children:"Task\u2011Level Only"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsx)(s.p,{children:"Rewards are generated per task, not per session.\nA session can contain 100 tasks. Each task must be scored separately."}),(0,a.jsxs)(s.p,{children:["That is why ",(0,a.jsx)(s.code,{children:"task_start"})," and ",(0,a.jsx)(s.code,{children:"task_end"})," are required."]})]}),"\n",(0,a.jsx)(s.h2,{id:"long-tasks-and-memory",children:"Long Tasks And Memory"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsx)(s.p,{children:"If a task is too large to fit in the judge context:"}),(0,a.jsxs)(s.ul,{children:["\n",(0,a.jsx)(s.li,{children:"Marlo compresses the trajectory into a memory summary"}),"\n",(0,a.jsx)(s.li,{children:"The judge reads the summary instead of full events"}),"\n"]}),(0,a.jsx)(s.p,{children:"This keeps scoring stable even on very long tasks."})]}),"\n",(0,a.jsx)(s.h2,{id:"reasoning-usage",children:"Reasoning Usage"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsxs)(s.p,{children:["If your model provides reasoning, include it in ",(0,a.jsx)(s.code,{children:"llm_call.reasoning"}),".\nThe judge will use it as evidence when it is available."]}),(0,a.jsx)(s.p,{children:"If it is not available, the judge still runs, but with less signal."})]}),"\n",(0,a.jsx)(s.h2,{id:"when-rewards-run",children:"When Rewards Run"}),"\n",(0,a.jsxs)("div",{className:"marlo-card",style:{border:"1px solid #E6E6E6",borderRadius:"12px",padding:"14px",backgroundColor:"#FFFFFF"},children:[(0,a.jsx)(s.p,{children:"Rewards run after a task ends and you run the reward pipeline."}),(0,a.jsx)(s.p,{children:"Reward runs never crash the agent. Errors are stored as metadata."})]})]})}function h(e={}){const{wrapper:s}={...(0,d.R)(),...e.components};return s?(0,a.jsx)(s,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,s,r){r.d(s,{R:()=>t,x:()=>i});var n=r(6540);const a={},d=n.createContext(a);function t(e){const s=n.useContext(d);return n.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function i(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),n.createElement(d.Provider,{value:s},e.children)}}}]);